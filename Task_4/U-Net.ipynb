{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tf_keras.layers import Conv2D, MaxPooling2D, Input, UpSampling2D, Activation, Concatenate\n",
    "from tf_keras.models import Model, load_model\n",
    "from tf_keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1024, 1024)\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "    \"\"\"Encoder block: Conv2D -> ReLU -> Conv2D -> ReLU -> MaxPooling.\"\"\"\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input_tensor)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    p = MaxPooling2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(input_tensor, skip_tensor, num_filters):\n",
    "    \"\"\"Decoder block: UpSampling2D -> Conv2D -> Concatenate -> Conv2D -> ReLU.\"\"\"\n",
    "    x = UpSampling2D((2, 2))(input_tensor)\n",
    "    x = Conv2D(num_filters, 2, padding=\"same\")(x)\n",
    "    x = Concatenate()([x, skip_tensor])\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def unet_model(input_shape=(256, 256, 3), num_classes=2):\n",
    "    \"\"\"U-Net model.\"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Contracting Path\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "    s5, p5 = encoder_block(p4, 1024)\n",
    "    \n",
    "\n",
    "    # Bottleneck\n",
    "    b1 = Conv2D(2048, 3, padding=\"same\")(p5)\n",
    "    b1 = Activation(\"relu\")(b1)\n",
    "    b1 = Conv2D(2048, 3, padding=\"same\")(b1)\n",
    "    b1 = Activation(\"relu\")(b1)\n",
    "\n",
    "    # Expansive Path\n",
    "    d0 = decoder_block(b1, s5, 1024)\n",
    "    d1 = decoder_block(d0, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    # Output\n",
    "    outputs = Conv2D(num_classes, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"U-Net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained U-net model...\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "Unet_model_path = \"U-net_model1.h5\"\n",
    "\n",
    "# Check if the U-net model exists and load it, else train it\n",
    "if os.path.exists(Unet_model_path):\n",
    "    print(\"Loading pre-trained U-net model...\")\n",
    "    Unet_model = load_model(Unet_model_path)\n",
    "else:\n",
    "    print(\"Training U-net model...\")\n",
    "    # U-net model\n",
    "    Unet_model = unet_model(input_shape=input_shape.__add__((3,)), num_classes=2)\n",
    "\n",
    "    # Save the trained U-net model\n",
    "    Unet_model.save(Unet_model_path)\n",
    "    print(\"U-net model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 28s 28s/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_path = \"loli.png\"\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = Image.open(img_path).resize(input_shape)\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array[:, :, :3], axis=0) / 255.0\n",
    "\n",
    "# Predict\n",
    "pred = Unet_model.predict(img_array)\n",
    "\n",
    "# Post-process prediction\n",
    "pred = np.squeeze(pred, axis=0)  # Remove batch dimension\n",
    "pred = np.argmax(pred, axis=-1)  # Remove channel dimension if it exists\n",
    "pred = Image.fromarray(np.uint8(pred*255))  # Convert to grayscale image  \n",
    "pred = pred.resize((img.width, img.height))  # Resize back to original dimensions\n",
    "\n",
    "plt.imshow(pred, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
